{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 04:41:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/28 04:41:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import countDistinct\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 04:41:39 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.7 (default, Sep 16 2021 08:50:36)\n",
      "Spark context Web UI available at http://192.168.178.88:4041\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1672198899311).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col, bround\n",
    "spark = SparkSession.builder \\\n",
    "         .appName('SparkByExamples.com') \\\n",
    "         .getOrCreate()\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark import Row\n",
    "from pyspark.shell import spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseude Code:\n",
    "\n",
    "agrupar por user, device, gt\n",
    "    para x, y, z sacar\n",
    "        median\n",
    "        desv\n",
    "        max\n",
    "        min\n",
    "\n",
    "join data from different datasets into RDDs\n",
    "Analize RDDs\n",
    "\n",
    "DonÂ´t repeat code!\n",
    "avoid many (unnecessary) lines of code / repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa = 'Phones_accelerometer.csv'\n",
    "pg = 'Phones_gyroscope.csv'\n",
    "wa = 'Watch_accelerometer.csv'\n",
    "wg = 'Watch_gyroscope.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"index\", \"arrival_time\", \"creation_time\", \"x\", \"y\", \"z\", \"user\", \"model\", \"device\", \"activity\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define which columns should be float for preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_float = [\"x\", \"y\", \"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_DF(file_name):\n",
    "    \"\"\"\n",
    "    This function takes in the file names and puts out the pre-processed dateframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # read file in pyspark context\n",
    "    read_file = sc.textFile(file_name)\n",
    "    # split into columns and convert to DF\n",
    "    DF = read_file.map(lambda line: [x for x in line.split(',')]).toDF(columns)\n",
    "    \n",
    "    #drop unnecessary columns\n",
    "    DF_prep = DF.drop(\"index\", \"arrival_time\", \"creation_time\", \"device\")\n",
    "    #convert numerical columns to float\n",
    "    for column in to_float:\n",
    "        DF_prep = DF_prep.withColumn(column, col(column).cast('float'))\n",
    "\n",
    "    return DF_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"key\" defines, which columns make up the key for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = [\"user\", \"model\", \"activity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"values\" defines, which columns should contain numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [\"avg(x)\", \"avg(y)\", \"avg(z)\", \"stddev(x)\", \"stddev(y)\", \"stddev(z)\", \"max(x)\", \"max(y)\", \"max(z)\", \"min(x)\", \"min(y)\", \"min(z)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/28 07:15:16 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 285134 ms exceeds timeout 120000 ms\n",
      "22/12/28 07:15:16 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "def analize(input):\n",
    "  \"\"\"\n",
    "  This function takes in the pre-processed data frame and performs the mathematical analysis:\n",
    "  \"\"\"\n",
    "\n",
    "  #compute the mean, standard deviation, max, and min by aggregation on the key\n",
    "  avg = input.groupBy(key).agg({\"x\":\"avg\", \"y\":\"avg\", \"z\":\"avg\"})\n",
    "  std = input.groupBy(key).agg({\"x\":\"stddev\", \"y\":\"stddev\", \"z\":\"stddev\"})\n",
    "  max = input.groupBy(key).agg({\"x\":\"max\", \"y\":\"max\", \"z\":\"max\"}) \n",
    "  min = input.groupBy(key).agg({\"x\":\"min\", \"y\":\"min\", \"z\":\"min\"})\n",
    "\n",
    "  #join individual outcomes\n",
    "  out = avg.join(std,key).join(max, key).join(min, key)\n",
    "\n",
    "  #transform numerical values to double data type and round to 5 decimal places\n",
    "  for col in values:\n",
    "    out = out.withColumn(col, f.round(f.col(col).cast(\"double\"), 5))\n",
    "\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    \"\"\"\n",
    "    Bundle the pre-processing and analytics functions.\n",
    "    \"\"\"\n",
    "    pa_prep = preprocess_DF(df)\n",
    "    out_pa = analize(pa_prep)\n",
    "\n",
    "    return out_pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply processing function to all data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+-------+--------+--------+---------+---------+---------+-------+--------+--------+--------+--------+--------+\n",
      "|user|model|activity| avg(x)|  avg(y)|  avg(z)|stddev(x)|stddev(y)|stddev(z)| max(x)|  max(y)|  max(z)|  min(x)|  min(y)|  min(z)|\n",
      "+----+-----+--------+-------+--------+--------+---------+---------+---------+-------+--------+--------+--------+--------+--------+\n",
      "|   a| gear|   stand|0.02215|-0.03287|-0.07144|  0.07316|  0.05571|  0.05018| 0.8104| 0.35447| 1.14756|-2.03838|-0.55287|-1.23198|\n",
      "|   a| gear|    null|0.02257|-0.03661|-0.07169|  0.01783|  0.01158|  0.00805|0.05619|-0.01811|-0.05513|-0.03143|-0.08043|-0.09348|\n",
      "|   a| gear|     sit|0.02271|-0.04142|-0.06997|  0.05704|  0.05628|  0.04524|0.39868| 0.39921| 0.77605|-0.70094|-0.79043| -0.2285|\n",
      "+----+-----+--------+-------+--------+--------+---------+---------+---------+-------+--------+--------+--------+--------+--------+\n",
      "\n",
      "+----+------+--------+-------+-------+------+---------+---------+---------+-------+-------+-------+-------+-------+--------+\n",
      "|user| model|activity| avg(x)| avg(y)|avg(z)|stddev(x)|stddev(y)|stddev(z)| max(x)| max(y)| max(z)| min(x)| min(y)|  min(z)|\n",
      "+----+------+--------+-------+-------+------+---------+---------+---------+-------+-------+-------+-------+-------+--------+\n",
      "|   a|nexus4|   stand|0.00159|0.00101|4.4E-4|  0.04278|  0.02861|  0.04594|0.63219|0.34972|0.44873|-0.1657|-0.1555|-0.60016|\n",
      "+----+------+--------+-------+-------+------+---------+---------+---------+-------+-------+-------+-------+-------+--------+\n",
      "\n",
      "+----+-----+--------+--------+--------+--------+---------+---------+---------+--------+--------+--------+---------+---------+--------+\n",
      "|user|model|activity|  avg(x)|  avg(y)|  avg(z)|stddev(x)|stddev(y)|stddev(z)|  max(x)|  max(y)|  max(z)|   min(x)|   min(y)|  min(z)|\n",
      "+----+-----+--------+--------+--------+--------+---------+---------+---------+--------+--------+--------+---------+---------+--------+\n",
      "|   a| gear|   stand|-9.28975|-3.13716|-1.06629|  0.41375|  1.06296|  0.61739|-0.56503| -0.5782| 1.01574|-12.60068|-11.08276|-2.26252|\n",
      "|   a| gear|    null|-9.26013|-3.47263|-1.04072|  0.03114|  0.05153|  0.03148|-9.17398|-3.32076|-0.93075| -9.35834|  -3.5943|-1.14203|\n",
      "|   a| gear|     sit|-7.60452|-5.52834| 2.63833|  0.18557|   0.2743|  0.38572|-6.65887|-0.83258| 3.55599|-10.82299| -6.91685|-3.67091|\n",
      "+----+-----+--------+--------+--------+--------+---------+---------+---------+--------+--------+--------+---------+---------+--------+\n",
      "\n",
      "+----+------+--------+-------+------+-------+---------+---------+---------+-------+-------+-------+--------+--------+-------+\n",
      "|user| model|activity| avg(x)|avg(y)| avg(z)|stddev(x)|stddev(y)|stddev(z)| max(x)| max(y)| max(z)|  min(x)|  min(y)| min(z)|\n",
      "+----+------+--------+-------+------+-------+---------+---------+---------+-------+-------+-------+--------+--------+-------+\n",
      "|   a|nexus4|   stand|-6.0265|0.9335|8.01365|  0.18456|  0.24045|  0.17601|-5.5202|1.94728|8.63879|-7.04483|-0.84251|7.14987|\n",
      "+----+------+--------+-------+------+-------+---------+---------+---------+-------+-------+-------+--------+--------+-------+\n",
      "\n",
      "None None None None\n"
     ]
    }
   ],
   "source": [
    "out_wg = process(wg)\n",
    "out_pg = process(pg)\n",
    "out_wa = process(wa)\n",
    "out_pa = process(pa)\n",
    "\n",
    "print(\n",
    "    out_wg.show(),\n",
    "    out_pg.show(),\n",
    "    out_wa.show(),\n",
    "    out_pa.show(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5c7435bc3a0af7c2e50ff20f20879f4ae59d0d746704ca49b2ca7888dc9b87d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
